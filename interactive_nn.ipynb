{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd #csv-files and tables\n",
    "import keras #neural network\n",
    "\n",
    "#create graphics\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "#utils for working with neural network\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import plot_model, to_categorical\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "#interactive in jupyter notebook\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activate widgets in terminal\n",
    "# jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading datasets\n",
    "dataset_0 = pd.read_csv('data/simple.csv')\n",
    "dataset_0.name = 'simple'\n",
    "dataset_1 = pd.read_csv('data/iris_new.csv')\n",
    "dataset_1.name = 'iris'\n",
    "dataset_2 = pd.read_csv('data/cancer.csv')\n",
    "dataset_2.name = 'cancer'\n",
    "dataset_3 = pd.read_csv('data/glass.csv')\n",
    "dataset_3.name = 'glass'\n",
    "dataset_4 = pd.read_csv('data/thyroid.csv')\n",
    "dataset_4.name = 'thyroid'\n",
    "dataset_5 = pd.read_csv('data/vine.csv')\n",
    "dataset_5.name = 'vine'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data markup\n",
    "#iloc -> select data on table\n",
    "\n",
    "input_data_0 = dataset_0.iloc[:, :2]\n",
    "output_data_0 = dataset_0.iloc[:, -4:]\n",
    "io0 = [input_data_0.values, output_data_0.values]\n",
    "\n",
    "input_data_1 = dataset_1.iloc[:, :4]\n",
    "output_data_1 = dataset_1.iloc[:, -3:]\n",
    "io1 = [input_data_1.values, output_data_1.values]\n",
    "\n",
    "input_data_2 = dataset_2.iloc[:, :9]\n",
    "output_data_2 = dataset_2.iloc[:, -2:]\n",
    "io2 = [input_data_2.values, output_data_2.values]\n",
    "\n",
    "input_data_3 = dataset_3.iloc[:, :9]\n",
    "output_data_3 = dataset_3.iloc[:, -2:]\n",
    "io3 = [input_data_3.values, output_data_3.values]\n",
    "\n",
    "input_data_4 = dataset_4.iloc[:, :21]\n",
    "output_data_4 = dataset_4.iloc[:, -3:]\n",
    "io4 = [input_data_4.values, output_data_4.values]\n",
    "\n",
    "input_data_5 = dataset_5.iloc[:, :13]\n",
    "output_data_5 = dataset_5.iloc[:, -3:]\n",
    "io5 = [input_data_5.values, output_data_5.values]\n",
    "\n",
    "io = [io0, io1, io2, io3, io4, io5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary selection by name\n",
    "dict_datasets = {dataset_0.name:(dataset_0,io0),\n",
    "                 dataset_1.name:(dataset_1,io1),\n",
    "                 dataset_2.name:(dataset_2,io2),\n",
    "                 dataset_3.name:(dataset_3,io3),\n",
    "                 dataset_4.name:(dataset_4,io4),\n",
    "                 dataset_5.name:(dataset_5,io4)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a401651045e44929b0fcc4ec3369c903",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='dataset_name', options=('simple', 'iris', 'cancer', 'glass', 'thyrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact_manual\n",
    "def learning_datasets(dataset_name = dict_datasets.keys(),\n",
    "                  test_size = (0.1,1,0.1),\n",
    "                  activation_1_layer = ['tanh', 'sigmoid'],\n",
    "                  activation_2_layer = ['tanh', 'sigmoid'],\n",
    "                  neurons_1_layer = (5,100,5),\n",
    "                  neurons_2_layer = (5,100,5),\n",
    "                  optimizer = ['adam', 'sgd'],\n",
    "                  loss = ['mse'],\n",
    "                  metrics = ['accuracy'],\n",
    "                  epochs = (1,101,5)):\n",
    "    \n",
    "    head = dict_datasets[dataset_name][0].head()   #first 5 rows in dataset\n",
    "    Q = dict_datasets[dataset_name][1][0].shape[0] #rows -> int\n",
    "    m = dict_datasets[dataset_name][1][0].shape[1] #input columns -> int\n",
    "    p = dict_datasets[dataset_name][1][1].shape[1] #output columns -> int\n",
    "    \n",
    "    input_data = dict_datasets[dataset_name][1][0]\n",
    "    output_data = dict_datasets[dataset_name][1][1]\n",
    "    \n",
    "    #split data into test and training samples\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(input_data, \n",
    "                                                        output_data,\n",
    "                                                        test_size=test_size)\n",
    "    \n",
    "    print(f'{Q} sample with {m} input columns and {p} output')\n",
    "    print(f'Size of train sample: {len(X_train)}')\n",
    "    print(f'Size of test sample: {len(X_test)}')\n",
    "    \n",
    "    #create model neural network\n",
    "    model = Sequential([\n",
    "                    Dense(neurons_1_layer, \n",
    "                          input_dim=m,\n",
    "                          activation=activation_1_layer,\n",
    "                          kernel_initializer='glorot_uniform'), #xavier initialization\n",
    "                    Dense(neurons_2_layer, \n",
    "                          activation=activation_2_layer,\n",
    "                          kernel_initializer='glorot_uniform'),\n",
    "                    Dense(p, \n",
    "                          activation='softmax', \n",
    "                          kernel_initializer='glorot_uniform'),\n",
    "                    ])\n",
    "    model.summary()\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss=loss,\n",
    "                  metrics=[metrics])\n",
    "    \n",
    "    x_axis = np.linspace(1, epochs, epochs)\n",
    "    \n",
    "    history = model.fit(X_train, Y_train, validation_split=0.1, epochs=epochs, verbose=0)\n",
    "    error_num_val = [int(np.ceil((100*(1 - i))/(100./len(X_test)))) for i in history.history['val_accuracy']]\n",
    "    \n",
    "    fig1 = go.Figure()\n",
    "    fig2 = go.Figure()\n",
    "    fig3 = go.Figure()\n",
    "    fig1.add_trace(go.Scatter(x=x_axis,\n",
    "                             y=history.history['loss'],\n",
    "                             name='Train sample'))\n",
    "    fig1.add_trace(go.Scatter(x=x_axis,\n",
    "                             y=history.history['val_loss'],\n",
    "                             name='Valid sample'))\n",
    "    fig2.add_trace(go.Scatter(x=x_axis,\n",
    "                             y=history.history['accuracy'],\n",
    "                             name='Train sample'))\n",
    "    fig2.add_trace(go.Scatter(x=x_axis,\n",
    "                             y=history.history['val_accuracy'],\n",
    "                             name='Valid sample'))\n",
    "    fig3.add_trace(go.Scatter(x=x_axis,\n",
    "                             y=error_num_val,\n",
    "                             name='Valid sample'))\n",
    "    fig1.update_layout(title='Loss',\n",
    "                       autosize=False,\n",
    "                       width=800,\n",
    "                       height=500,\n",
    "                       yaxis=dict(title_text='loss'),\n",
    "                       xaxis=dict(title_text='Epoch'))\n",
    "    fig2.update_layout(title='Accuracy',\n",
    "                       autosize=False,\n",
    "                       width=800,\n",
    "                       height=500,\n",
    "                       yaxis=dict(title_text='Accuracy'),\n",
    "                       xaxis=dict(title_text='Epoch'))\n",
    "    fig3.update_layout(title='Errors',\n",
    "                       autosize=False,\n",
    "                       width=800,\n",
    "                       height=500,\n",
    "                       yaxis=dict(title_text='Errors'),\n",
    "                       xaxis=dict(title_text='Epoch'))\n",
    "    fig1.show()\n",
    "    fig2.show()\n",
    "    fig3.show()\n",
    "        \n",
    "    ev = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    error_num_end = int(np.ceil((100*(1 - ev[1]))/(100./len(X_test))))\n",
    "    \n",
    "    print(f'Accuracy test sample: {ev[1]}')\n",
    "    print(f'Errors : {error_num_end} ')\n",
    "    \n",
    "    return head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
