{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd #csv-files and tables\n",
    "import tensorflow as tf #neural network\n",
    "# import keras.backend as K\n",
    "#create graphics\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "# #utils for working with neural network\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# #interactive in jupyter notebook\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import levenberg_marquardt as lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activate widgets in terminal\n",
    "# jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading datasets\n",
    "dataset_0 = pd.read_csv('data/simple.csv')\n",
    "dataset_0.name = 'simple'\n",
    "dataset_1 = pd.read_csv('data/iris_new.csv')\n",
    "dataset_1.name = 'iris'\n",
    "dataset_2 = pd.read_csv('data/cancer.csv')\n",
    "dataset_2.name = 'cancer'\n",
    "dataset_3 = pd.read_csv('data/glass.csv')\n",
    "dataset_3.name = 'glass'\n",
    "dataset_4 = pd.read_csv('data/thyroid.csv')\n",
    "dataset_4.name = 'thyroid'\n",
    "dataset_5 = pd.read_csv('data/vine.csv')\n",
    "dataset_5.name = 'vine'\n",
    "dataset_6 = pd.read_csv('data/train_hackaton.csv')\n",
    "dataset_6.name = 'hackaton'\n",
    "dataset_7 = pd.read_excel('data/data_spectrum1.xlsx', sheet_name=\"Лист2\")\n",
    "dataset_7.name = 'sensor'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test_hack = pd.read_csv('data/test_hackaton.csv')\n",
    "input_data_test_hack = dataset_test_hack.iloc[:, :22]\n",
    "output_data_test_hack = dataset_test_hack.iloc[:, -5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_test_hack = []\n",
    "for i in output_data_test_hack.values:\n",
    "    max_test_hack.append(tf.math.argmax(i).numpy() + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data markup\n",
    "#iloc -> select data on table\n",
    "\n",
    "input_data_0 = dataset_0.iloc[:, :2]\n",
    "output_data_0 = dataset_0.iloc[:, -4:]\n",
    "io0 = [input_data_0.values, output_data_0.values]\n",
    "\n",
    "input_data_1 = dataset_1.iloc[:, :4]\n",
    "output_data_1 = dataset_1.iloc[:, -3:]\n",
    "io1 = [input_data_1.values, output_data_1.values]\n",
    "\n",
    "input_data_2 = dataset_2.iloc[:, :9]\n",
    "output_data_2 = dataset_2.iloc[:, -2:]\n",
    "io2 = [input_data_2.values, output_data_2.values]\n",
    "\n",
    "input_data_3 = dataset_3.iloc[:, :9]\n",
    "output_data_3 = dataset_3.iloc[:, -2:]\n",
    "io3 = [input_data_3.values, output_data_3.values]\n",
    "\n",
    "input_data_4 = dataset_4.iloc[:, :21]\n",
    "output_data_4 = dataset_4.iloc[:, -3:]\n",
    "io4 = [input_data_4.values, output_data_4.values]\n",
    "\n",
    "input_data_5 = dataset_5.iloc[:, :13]\n",
    "output_data_5 = dataset_5.iloc[:, -3:]\n",
    "io5 = [input_data_5.values, output_data_5.values]\n",
    "\n",
    "input_data_6 = dataset_6.iloc[:, :22]\n",
    "output_data_6 = dataset_6.iloc[:, -5:]\n",
    "io6 = [input_data_6.values, output_data_6.values]\n",
    "\n",
    "input_data_7 = dataset_7.iloc[:, 10:40]\n",
    "output_data_7 = dataset_7.iloc[:, -1:]\n",
    "io7 = [input_data_7.values, output_data_7.values]\n",
    "\n",
    "# io = [io0, io1, io2, io3, io4, io5, io6, io7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_7 = input_data_7/input_data_7.max().astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "io = [io0, io1, io2, io3, io4, io5, io6, io7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>...</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.005276</td>\n",
       "      <td>0.008604</td>\n",
       "      <td>0.007849</td>\n",
       "      <td>0.008226</td>\n",
       "      <td>0.006442</td>\n",
       "      <td>0.003938</td>\n",
       "      <td>0.002178</td>\n",
       "      <td>0.001895</td>\n",
       "      <td>0.001382</td>\n",
       "      <td>0.000454</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002693</td>\n",
       "      <td>0.003245</td>\n",
       "      <td>0.004920</td>\n",
       "      <td>0.006613</td>\n",
       "      <td>0.003310</td>\n",
       "      <td>0.003278</td>\n",
       "      <td>0.000912</td>\n",
       "      <td>0.000631</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.001468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.005221</td>\n",
       "      <td>0.009385</td>\n",
       "      <td>0.008679</td>\n",
       "      <td>0.010040</td>\n",
       "      <td>0.007263</td>\n",
       "      <td>0.004398</td>\n",
       "      <td>0.002522</td>\n",
       "      <td>0.001787</td>\n",
       "      <td>0.001193</td>\n",
       "      <td>0.000406</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002198</td>\n",
       "      <td>0.004092</td>\n",
       "      <td>0.005739</td>\n",
       "      <td>0.007054</td>\n",
       "      <td>0.003637</td>\n",
       "      <td>0.003120</td>\n",
       "      <td>0.001120</td>\n",
       "      <td>0.000856</td>\n",
       "      <td>0.001065</td>\n",
       "      <td>0.004032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.004442</td>\n",
       "      <td>0.007017</td>\n",
       "      <td>0.006907</td>\n",
       "      <td>0.005904</td>\n",
       "      <td>0.004471</td>\n",
       "      <td>0.002561</td>\n",
       "      <td>0.001731</td>\n",
       "      <td>0.001480</td>\n",
       "      <td>0.001323</td>\n",
       "      <td>0.000393</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002085</td>\n",
       "      <td>0.003364</td>\n",
       "      <td>0.004413</td>\n",
       "      <td>0.004588</td>\n",
       "      <td>0.002455</td>\n",
       "      <td>0.002501</td>\n",
       "      <td>0.000586</td>\n",
       "      <td>0.000501</td>\n",
       "      <td>0.000355</td>\n",
       "      <td>0.001232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.004199</td>\n",
       "      <td>0.007661</td>\n",
       "      <td>0.007560</td>\n",
       "      <td>0.006579</td>\n",
       "      <td>0.005481</td>\n",
       "      <td>0.003341</td>\n",
       "      <td>0.002015</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.001509</td>\n",
       "      <td>0.000569</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002312</td>\n",
       "      <td>0.004386</td>\n",
       "      <td>0.003993</td>\n",
       "      <td>0.004700</td>\n",
       "      <td>0.002277</td>\n",
       "      <td>0.002858</td>\n",
       "      <td>0.001057</td>\n",
       "      <td>0.000577</td>\n",
       "      <td>0.000771</td>\n",
       "      <td>0.001706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.003508</td>\n",
       "      <td>0.005094</td>\n",
       "      <td>0.003929</td>\n",
       "      <td>0.005116</td>\n",
       "      <td>0.003496</td>\n",
       "      <td>0.001977</td>\n",
       "      <td>0.001430</td>\n",
       "      <td>0.001222</td>\n",
       "      <td>0.000826</td>\n",
       "      <td>0.000324</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001036</td>\n",
       "      <td>0.002035</td>\n",
       "      <td>0.002894</td>\n",
       "      <td>0.004009</td>\n",
       "      <td>0.001559</td>\n",
       "      <td>0.001304</td>\n",
       "      <td>0.000538</td>\n",
       "      <td>0.000252</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>0.000671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.019447</td>\n",
       "      <td>0.028098</td>\n",
       "      <td>0.032014</td>\n",
       "      <td>0.029893</td>\n",
       "      <td>0.037557</td>\n",
       "      <td>0.032167</td>\n",
       "      <td>0.023544</td>\n",
       "      <td>0.013258</td>\n",
       "      <td>0.011546</td>\n",
       "      <td>0.003037</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013473</td>\n",
       "      <td>0.021843</td>\n",
       "      <td>0.028922</td>\n",
       "      <td>0.028662</td>\n",
       "      <td>0.014448</td>\n",
       "      <td>0.011207</td>\n",
       "      <td>0.002171</td>\n",
       "      <td>0.001676</td>\n",
       "      <td>0.001157</td>\n",
       "      <td>0.005018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.002491</td>\n",
       "      <td>0.003409</td>\n",
       "      <td>0.004539</td>\n",
       "      <td>0.005485</td>\n",
       "      <td>0.005503</td>\n",
       "      <td>0.004282</td>\n",
       "      <td>0.001994</td>\n",
       "      <td>0.001738</td>\n",
       "      <td>0.001101</td>\n",
       "      <td>0.000269</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000960</td>\n",
       "      <td>0.001615</td>\n",
       "      <td>0.001659</td>\n",
       "      <td>0.001630</td>\n",
       "      <td>0.001778</td>\n",
       "      <td>0.001783</td>\n",
       "      <td>0.000418</td>\n",
       "      <td>0.000290</td>\n",
       "      <td>0.000259</td>\n",
       "      <td>0.003525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.001581</td>\n",
       "      <td>0.001980</td>\n",
       "      <td>0.004061</td>\n",
       "      <td>0.003021</td>\n",
       "      <td>0.004284</td>\n",
       "      <td>0.002883</td>\n",
       "      <td>0.001157</td>\n",
       "      <td>0.001034</td>\n",
       "      <td>0.000729</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001473</td>\n",
       "      <td>0.001714</td>\n",
       "      <td>0.004360</td>\n",
       "      <td>0.002256</td>\n",
       "      <td>0.001387</td>\n",
       "      <td>0.002271</td>\n",
       "      <td>0.000827</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>0.000447</td>\n",
       "      <td>0.000823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.003915</td>\n",
       "      <td>0.009253</td>\n",
       "      <td>0.028400</td>\n",
       "      <td>0.023425</td>\n",
       "      <td>0.031681</td>\n",
       "      <td>0.032678</td>\n",
       "      <td>0.010555</td>\n",
       "      <td>0.010536</td>\n",
       "      <td>0.004481</td>\n",
       "      <td>0.003316</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003538</td>\n",
       "      <td>0.008033</td>\n",
       "      <td>0.016328</td>\n",
       "      <td>0.007220</td>\n",
       "      <td>0.003799</td>\n",
       "      <td>0.004621</td>\n",
       "      <td>0.001363</td>\n",
       "      <td>0.000675</td>\n",
       "      <td>0.000536</td>\n",
       "      <td>0.002398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.009128</td>\n",
       "      <td>0.010193</td>\n",
       "      <td>0.013307</td>\n",
       "      <td>0.022532</td>\n",
       "      <td>0.034173</td>\n",
       "      <td>0.016337</td>\n",
       "      <td>0.015178</td>\n",
       "      <td>0.010621</td>\n",
       "      <td>0.004713</td>\n",
       "      <td>0.001379</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005108</td>\n",
       "      <td>0.012493</td>\n",
       "      <td>0.006716</td>\n",
       "      <td>0.009984</td>\n",
       "      <td>0.004946</td>\n",
       "      <td>0.004837</td>\n",
       "      <td>0.001428</td>\n",
       "      <td>0.001614</td>\n",
       "      <td>0.001121</td>\n",
       "      <td>0.002857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>95 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          11        12        13        14        15        16        17  \\\n",
       "0   0.005276  0.008604  0.007849  0.008226  0.006442  0.003938  0.002178   \n",
       "1   0.005221  0.009385  0.008679  0.010040  0.007263  0.004398  0.002522   \n",
       "2   0.004442  0.007017  0.006907  0.005904  0.004471  0.002561  0.001731   \n",
       "3   0.004199  0.007661  0.007560  0.006579  0.005481  0.003341  0.002015   \n",
       "4   0.003508  0.005094  0.003929  0.005116  0.003496  0.001977  0.001430   \n",
       "..       ...       ...       ...       ...       ...       ...       ...   \n",
       "90  0.019447  0.028098  0.032014  0.029893  0.037557  0.032167  0.023544   \n",
       "91  0.002491  0.003409  0.004539  0.005485  0.005503  0.004282  0.001994   \n",
       "92  0.001581  0.001980  0.004061  0.003021  0.004284  0.002883  0.001157   \n",
       "93  0.003915  0.009253  0.028400  0.023425  0.031681  0.032678  0.010555   \n",
       "94  0.009128  0.010193  0.013307  0.022532  0.034173  0.016337  0.015178   \n",
       "\n",
       "          18        19        20  ...        23        24        25        26  \\\n",
       "0   0.001895  0.001382  0.000454  ...  0.002693  0.003245  0.004920  0.006613   \n",
       "1   0.001787  0.001193  0.000406  ...  0.002198  0.004092  0.005739  0.007054   \n",
       "2   0.001480  0.001323  0.000393  ...  0.002085  0.003364  0.004413  0.004588   \n",
       "3   0.002400  0.001509  0.000569  ...  0.002312  0.004386  0.003993  0.004700   \n",
       "4   0.001222  0.000826  0.000324  ...  0.001036  0.002035  0.002894  0.004009   \n",
       "..       ...       ...       ...  ...       ...       ...       ...       ...   \n",
       "90  0.013258  0.011546  0.003037  ...  0.013473  0.021843  0.028922  0.028662   \n",
       "91  0.001738  0.001101  0.000269  ...  0.000960  0.001615  0.001659  0.001630   \n",
       "92  0.001034  0.000729  0.000143  ...  0.001473  0.001714  0.004360  0.002256   \n",
       "93  0.010536  0.004481  0.003316  ...  0.003538  0.008033  0.016328  0.007220   \n",
       "94  0.010621  0.004713  0.001379  ...  0.005108  0.012493  0.006716  0.009984   \n",
       "\n",
       "          27        28        29        30        31        32  \n",
       "0   0.003310  0.003278  0.000912  0.000631  0.000476  0.001468  \n",
       "1   0.003637  0.003120  0.001120  0.000856  0.001065  0.004032  \n",
       "2   0.002455  0.002501  0.000586  0.000501  0.000355  0.001232  \n",
       "3   0.002277  0.002858  0.001057  0.000577  0.000771  0.001706  \n",
       "4   0.001559  0.001304  0.000538  0.000252  0.000220  0.000671  \n",
       "..       ...       ...       ...       ...       ...       ...  \n",
       "90  0.014448  0.011207  0.002171  0.001676  0.001157  0.005018  \n",
       "91  0.001778  0.001783  0.000418  0.000290  0.000259  0.003525  \n",
       "92  0.001387  0.002271  0.000827  0.000399  0.000447  0.000823  \n",
       "93  0.003799  0.004621  0.001363  0.000675  0.000536  0.002398  \n",
       "94  0.004946  0.004837  0.001428  0.001614  0.001121  0.002857  \n",
       "\n",
       "[95 rows x 22 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary selection by name\n",
    "dict_datasets = {\n",
    " dataset_0.name:(dataset_0,io0),\n",
    " dataset_1.name:(dataset_1,io1),\n",
    " dataset_2.name:(dataset_2,io2),\n",
    " dataset_3.name:(dataset_3,io3),\n",
    " dataset_4.name:(dataset_4,io4),\n",
    " dataset_5.name:(dataset_5,io5),\n",
    " dataset_6.name:(dataset_6,io6),\n",
    " dataset_7.name:(dataset_7,io7)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae_easy(y_true, y_pred):\n",
    "    return K.mean(K.abs(y_true-y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d74463eaebd4af8801e09108ae8e0d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='dataset_name', options=('simple', 'iris', 'cancer', 'glass', 'thyr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact_manual\n",
    "def learning_datasets(\n",
    "        dataset_name = dict_datasets.keys(),\n",
    "        test_size = (0.1,1,0.1),\n",
    "        batch_size = widgets.IntText(\n",
    "            value=4,\n",
    "            disabled=False\n",
    "        ),\n",
    "        activation_1_layer = ['tanh', 'sigmoid', 'relu'],\n",
    "        kernel_reg_1_layer = widgets.FloatText(\n",
    "            value=0.0,\n",
    "            disabled=False\n",
    "        ),\n",
    "        dropout_1 =  widgets.FloatText(\n",
    "            value=0.0,\n",
    "            disabled=False\n",
    "        ), \n",
    "        activation_2_layer = ['tanh', 'sigmoid', 'relu'],\n",
    "        kernel_reg_2_layer = widgets.FloatText(\n",
    "            value=0.0,\n",
    "            disabled=False\n",
    "        ),\n",
    "        dropout_2 =  widgets.FloatText(\n",
    "            value=0.0,\n",
    "            disabled=False\n",
    "        ),\n",
    "        neurons_1_layer = (5,100,5),\n",
    "        neurons_2_layer = (5,100,5),\n",
    "        optimizer = ['adam', 'sgd', 'RMSprop'],\n",
    "        learning_rate = widgets.FloatText(\n",
    "            value=0.1,\n",
    "            disabled=False\n",
    "        ),\n",
    "        loss = ['mse', 'mae', 'categorical_crossentropy'],\n",
    "        metrics = ['accuracy', 'mae'],\n",
    "        epochs = widgets.IntText(\n",
    "            value=100,\n",
    "            disabled=False\n",
    "        )\n",
    "    ):\n",
    "    \n",
    "    head = dict_datasets[dataset_name][0].head()   #first 5 rows in dataset\n",
    "    Q = dict_datasets[dataset_name][1][0].shape[0] #rows -> int\n",
    "    m = dict_datasets[dataset_name][1][0].shape[1] #input columns -> int\n",
    "    p = dict_datasets[dataset_name][1][1].shape[1] #output columns -> int\n",
    "    \n",
    "    input_data = dict_datasets[dataset_name][1][0]\n",
    "    output_data = dict_datasets[dataset_name][1][1]\n",
    "    \n",
    "    fig1 = go.Figure()\n",
    "    fig2 = go.Figure()\n",
    "    fig3 = go.Figure()\n",
    "    \n",
    "    x_axis = np.linspace(1, epochs, epochs)\n",
    "    val_metrics = f'val_{metrics}'\n",
    "    \n",
    "    list_loss = []\n",
    "    list_metric = []\n",
    "    \n",
    "    #create model neural network\n",
    "    for _ in range(1):\n",
    "        \n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "            input_data, \n",
    "            output_data,\n",
    "            test_size=test_size\n",
    "        )\n",
    "        X_train, X_test, Y_train, Y_test = X_train.astype(np.float32),X_test.astype(np.float32),Y_train.astype(np.float32),Y_test.astype(np.float32)\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(\n",
    "                neurons_1_layer, \n",
    "                input_dim=m,\n",
    "                activation=activation_1_layer,\n",
    "                kernel_regularizer = tf.keras.regularizers.l2(float(kernel_reg_1_layer)),\n",
    "                kernel_initializer='glorot_uniform' #xavier initialization\n",
    "            ),\n",
    "#             tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Dropout(dropout_1),\n",
    "            tf.keras.layers.Dense(\n",
    "                neurons_2_layer, \n",
    "                activation=activation_2_layer,\n",
    "                kernel_regularizer = tf.keras.regularizers.l2(float(kernel_reg_2_layer)),\n",
    "                kernel_initializer='glorot_uniform'\n",
    "            ),\n",
    "#             tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Dropout(dropout_2),\n",
    "            tf.keras.layers.Dense(\n",
    "                1, \n",
    "                activation='sigmoid',\n",
    "                kernel_initializer='glorot_uniform',\n",
    "            ),\n",
    "        ])\n",
    "        \n",
    "#     error_num_val = [int(np.ceil((100*(1 - i))/(100./len(X_test)))) for i in history.history['val_accuracy']]\n",
    "        \n",
    "        model.compile(optimizer=optimizer,\n",
    "#                   loss=[tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.SUM)],\n",
    "                      loss = [loss],\n",
    "#                   metrics=[tf.keras.losses.MeanAbsoluteError(reduction=tf.keras.losses.Reduction.SUM)]\n",
    "#                     metrics = [tf.keras.metrics.MeanAbsoluteError()]\n",
    "                      metrics = [metrics]\n",
    "                )\n",
    "        \n",
    "        history = model.fit(\n",
    "            X_train,\n",
    "            Y_train,\n",
    "#             validation_split=0.1,\n",
    "#             validation_data=(input_data_test_hack.values, output_data_test_hack.values),\n",
    "            validation_data=(X_test, Y_test),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            verbose=0\n",
    "        )\n",
    "#         print(history.history)\n",
    "        fig1.add_trace(\n",
    "            go.Scatter(\n",
    "                x=x_axis,\n",
    "                y=history.history['loss'],\n",
    "                name=f'Train sample {_}',\n",
    "                 line=dict(color='green')\n",
    "            )\n",
    "        )\n",
    "        fig1.add_trace(\n",
    "            go.Scatter(\n",
    "                x=x_axis,\n",
    "                y=history.history['val_loss'],\n",
    "                name=f'Valid sample {_}',\n",
    "                line=dict(color='black')\n",
    "            )\n",
    "        )\n",
    "        fig2.add_trace(\n",
    "            go.Scatter(\n",
    "                x=x_axis,\n",
    "#                 y=history.history['mean_absolute_error'],\n",
    "                y=history.history[metrics],\n",
    "                name=f'Train sample {_}',\n",
    "                line=dict(color='green')\n",
    "            )\n",
    "        )\n",
    "        fig2.add_trace(\n",
    "            go.Scatter(\n",
    "                x=x_axis,\n",
    "#                 y=history.history['val_mean_absolute_error'],\n",
    "                y=history.history[val_metrics],\n",
    "                name=f'Valid sample {_}',\n",
    "                line=dict(color='black')\n",
    "            )\n",
    "        )\n",
    "#     fig3.add_trace(\n",
    "#         go.Scatter(\n",
    "#             x=x_axis,\n",
    "#             y=error_num_val,\n",
    "#             name='Valid sample'\n",
    "#         )\n",
    "#     )\n",
    "#     print(history.history)\n",
    "    \n",
    "        eval_step = model.evaluate(X_test, Y_test, batch_size=batch_size, verbose=0)\n",
    "        list_loss.append(eval_step[0])\n",
    "        list_metric.append(eval_step[1])\n",
    "    \n",
    "    fig1.update_layout(\n",
    "        title='Loss',\n",
    "        autosize=False,\n",
    "        width=800,\n",
    "        height=500,\n",
    "        yaxis=dict(title_text=f'{loss}'),\n",
    "        xaxis=dict(title_text='Epoch')\n",
    "    )\n",
    "    fig2.update_layout(\n",
    "        title='Metrics',\n",
    "        autosize=False,\n",
    "        width=800,\n",
    "        height=500,\n",
    "        yaxis=dict(title_text=f'{metrics}'),\n",
    "        xaxis=dict(title_text='Epoch')\n",
    "    )\n",
    "#     fig3.update_layout(\n",
    "#         title='Errors',\n",
    "#         autosize=False,\n",
    "#         width=800,\n",
    "#         height=500,\n",
    "#         yaxis=dict(title_text='Errors'),\n",
    "#         xaxis=dict(title_text='Epoch')\n",
    "#     )\n",
    "    \n",
    "    print(f'{Q} sample with {m} input columns and {p} output')\n",
    "    print(f'Size of train sample: {len(X_train)}')\n",
    "    print(f'Size of test sample: {len(X_test)}')\n",
    "    \n",
    "    fig1.show()\n",
    "    fig2.show()\n",
    "#     fig3.show()\n",
    "        \n",
    "    ev = model.evaluate(X_test, Y_test, batch_size=batch_size, verbose=0)\n",
    "    \n",
    "    predictions = model.predict(X_test)\n",
    "#     print(f'MAE test: {mean_absolute_error(predictions, Y_test)}')\n",
    "#     print(f'Accuracy test: {accuracy_score(predictions, Y_test)}')\n",
    "    \n",
    "    if dataset_name == 'hackaton':\n",
    "        predictions_test_hackaton = model.predict(input_data_test_hack)\n",
    "        pred_max = []\n",
    "        for i in predictions_test_hackaton:\n",
    "            pred_max.append(tf.math.argmax(i).numpy() + 1)\n",
    "        print(f'Prediction labels {pred_max}')\n",
    "        mae_sklearn_hack = mean_absolute_error(pred_max, max_test_hack)\n",
    "#         print(predictions_test_hackaton)\n",
    "        print(f'Sklearn test hackaton mae {mae_sklearn_hack}')\n",
    "    \n",
    "#     mae_sklearn = mean_absolute_error(predictions, Y_test)\n",
    "\n",
    "    #     error_num_end = int(np.ceil((100*(1 - ev[1]))/(100./len(X_test))))\n",
    "    \n",
    "#     print(f'Sklearn test mae {mae_sklearn}')\n",
    "#     print(f'{metrics} test samples: {ev}')\n",
    "\n",
    "    #     print(f'Errors : {error_num_end} ')\n",
    "    \n",
    "    list_loss_lm = []\n",
    "    list_metric_lm = []\n",
    "    \n",
    "    \n",
    "    print(f\"------------------ LEVENBERG-MARQUARDT -----------------\")\n",
    "    \n",
    "    fig4 = go.Figure()\n",
    "    fig5 = go.Figure()\n",
    "\n",
    "    for _ in range(5):\n",
    "        \n",
    "        model_wrapper = lm.ModelWrapper(tf.keras.models.clone_model(model))\n",
    "        model_wrapper.compile(\n",
    "            optimizer=tf.keras.optimizers.SGD(learning_rate=1.),\n",
    "            loss=lm.MeanSquaredError(),\n",
    "            metrics = [metrics]\n",
    "        )\n",
    "        history_lm = model_wrapper.fit(\n",
    "            X_train,\n",
    "            Y_train,\n",
    "            epochs=epochs,\n",
    "            verbose=0\n",
    "        )\n",
    "#         print(model_wrapper.train_step())\n",
    "#         print(history_lm)\n",
    "#         break\n",
    "        if dataset_name == 'hackaton':\n",
    "            pred_lm = model_wrapper.predict(input_data_test_hack)\n",
    "            pred_max_lm = []\n",
    "            for i in pred_lm:\n",
    "                pred_max_lm.append(tf.math.argmax(i).numpy() + 1)\n",
    "            print(f'Prediction labels lm {pred_max_lm}')\n",
    "            mae_sklearn_hack_lm = mean_absolute_error(pred_max_lm, max_test_hack)\n",
    "            print(f'Sklearn test hackaton mae lm {mae_sklearn_hack_lm}')\n",
    "        \n",
    "#         fig4.add_trace(\n",
    "#             go.Scatter(\n",
    "#                 x=x_axis,\n",
    "#                 y=history_lm.history['loss'],\n",
    "#                 name=f'Train sample {_}',\n",
    "#                  line=dict(color='green')\n",
    "#             )\n",
    "#         )\n",
    "#         fig4.add_trace(\n",
    "#             go.Scatter(\n",
    "#                 x=x_axis,\n",
    "#                 y=history_lm.history['val_loss'],\n",
    "#                 name=f'Valid sample {_}',\n",
    "#                 line=dict(color='black')\n",
    "#             )\n",
    "#         )\n",
    "#         fig5.add_trace(\n",
    "#             go.Scatter(\n",
    "#                 x=x_axis,\n",
    "# #                 y=history.history['mean_absolute_error'],\n",
    "#                 y=history_lm.history[metrics],\n",
    "#                 name=f'Train sample {_}',\n",
    "#                 line=dict(color='green')\n",
    "#             )\n",
    "#         )\n",
    "#         fig5.add_trace(\n",
    "#             go.Scatter(\n",
    "#                 x=x_axis,\n",
    "# #                 y=history.history['val_mean_absolute_error'],\n",
    "#                 y=history_lm.history[val_metrics],\n",
    "#                 name=f'Valid sample {_}',\n",
    "#                 line=dict(color='black')\n",
    "#             )\n",
    "#         )\n",
    "        \n",
    "        \n",
    "        ev_lm = model_wrapper.evaluate(X_test, Y_test, batch_size=batch_size, verbose=0)\n",
    "        list_loss_lm.append(ev_lm[0])\n",
    "        list_metric_lm.append(ev_lm[1])\n",
    "        \n",
    "        #     print(ev_lm)\n",
    "    \n",
    "#     fig4.update_layout(\n",
    "#         title='Loss',\n",
    "#         autosize=False,\n",
    "#         width=800,\n",
    "#         height=500,\n",
    "#         yaxis=dict(title_text=f'{loss}'),\n",
    "#         xaxis=dict(title_text='Epoch')\n",
    "#     )\n",
    "#     fig5.update_layout(\n",
    "#         title='Metrics',\n",
    "#         autosize=False,\n",
    "#         width=800,\n",
    "#         height=500,\n",
    "#         yaxis=dict(title_text=f'{metrics}'),\n",
    "#         xaxis=dict(title_text='Epoch')\n",
    "#     )\n",
    "    \n",
    "    print(f\"{optimizer} \")\n",
    "    print(f'MEAN {loss} {sum(list_loss)/len(list_loss)} MEAN {metrics} {sum(list_metric)/len(list_metric)}')\n",
    "    print(f'MAX {loss} {max(list_loss)} MAX {metrics} {max(list_metric)}')\n",
    "    print(f'MIN {loss} {min(list_loss)} MIN {metrics} {min(list_metric)}')\n",
    "    print(f\"LEVENBERG-MARQUARDT\")\n",
    "    print(f'MEAN {loss} {sum(list_loss_lm)/len(list_loss_lm)} MEAN {metrics} {sum(list_metric_lm)/len(list_metric_lm)}')\n",
    "    print(f'MAX {loss} {max(list_loss_lm)} MAX {metrics} {max(list_metric_lm)}')\n",
    "    print(f'MIN {loss} {min(list_loss_lm)} MIN {metrics} {min(list_metric_lm)}')\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
